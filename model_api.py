from contextlib import asynccontextmanager
from fastapi import FastAPI
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import time

model_path = os.path.abspath("./mpt-7b-storywriter")
os.environ["HUGGINGFACE_HUB_CACHE"] = model_path
model_name = "mosaicml/mpt-7b-storywriter"

# to make it lazily load
tokenizer = None
model = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global tokenizer, model
    print("Loading model and tokenizer...")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    print("Model and tokenizer loaded.")
    yield # server executing... 

    print("Cleaning up resources...") # coming back after server terminated
    del tokenizer 
    del model
    print("Resources cleaned up.")

app = FastAPI(lifespan=lifespan)

@app.get("/generate_txt")
async def generate_txt(prompt: str):   
    # prompt = data.get("prompt", "Three cats are") # if @app.post
    start_inference = time.perf_counter()

    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    output = model.generate(input_ids, max_length=5, num_return_sequences=1)

    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    
    inference_time = time.perf_counter() - start_inference
    print(f"Inference for prompt '{prompt}' took {inference_time:.2f} seconds.")
    
    return {
        "prompt": prompt,
        "generated_text": generated_text,
        "inference_time": inference_time
    }

@app.get("/")
async def health_check():
    return {"status": "OK"}
